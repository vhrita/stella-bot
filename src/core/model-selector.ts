import { readFileSync } from 'fs';
import { join } from 'path';
import { logger } from './logger.js';
import { isLocalAIOnline, getAvailableModels } from './local-ai.js';

// OTIMIZA√á√ÉO FASE 3: Cache de Status de Servi√ßos
interface ServiceStatusCache {
  isOnline: boolean;
  timestamp: number;
  lastCheck: number;
}

interface AIModel {
  name: string;
  slug: string;
  bestFor: string;
  pros: string;
  cons: string;
  keywords: string[];
  routingHints: string;
  providers: Array<{
    name: string;
    endpoint: string;
  }>;
  response: {
    url: { field: string };
    base64: { field: string };
  };
}

interface AIModelsConfig {
  models: AIModel[];
}

interface ModelOption {
  name: string;
  value: string;
  description?: string;
}

// OTIMIZA√á√ÉO FASE 3: Cache para status de servi√ßos
const serviceStatusCache = new Map<string, ServiceStatusCache>();
const CACHE_TTL = 30 * 1000; // 30 segundos
const MIN_CHECK_INTERVAL = 5 * 1000; // 5 segundos entre verifica√ß√µes

/**
 * OTIMIZA√á√ÉO FASE 3: Verifica√ß√£o de servi√ßo com cache
 */
async function checkServiceWithCache(serviceKey: string, checkFn: () => Promise<boolean>): Promise<boolean> {
  const now = Date.now();
  const cached = serviceStatusCache.get(serviceKey);
  
  // Verificar se cache √© v√°lido e se n√£o passou do intervalo m√≠nimo
  if (cached && 
      now - cached.timestamp < CACHE_TTL && 
      now - cached.lastCheck < MIN_CHECK_INTERVAL) {
    logger.debug(`üéØ Cache hit para ${serviceKey}: ${cached.isOnline}`);
    return cached.isOnline;
  }
  
  // Se est√° dentro do intervalo m√≠nimo mas cache expirou, retornar √∫ltimo status
  if (cached && now - cached.lastCheck < MIN_CHECK_INTERVAL) {
    logger.debug(`‚è∞ Aguardando intervalo para ${serviceKey}, usando cache: ${cached.isOnline}`);
    return cached.isOnline;
  }
  
  try {
    logger.debug(`üîÑ Verificando status de ${serviceKey}...`);
    const isOnline = await checkFn();
    
    // Atualizar cache
    serviceStatusCache.set(serviceKey, {
      isOnline,
      timestamp: now,
      lastCheck: now
    });
    
    logger.debug(`‚úÖ Status ${serviceKey}: ${isOnline ? 'online' : 'offline'}`);
    return isOnline;
  } catch (error) {
    logger.debug(`‚ùå Erro ao verificar ${serviceKey}:`, error);
    
    // Se h√° cache, usar o √∫ltimo status conhecido
    if (cached) {
      logger.debug(`üîÑ Usando √∫ltimo status conhecido para ${serviceKey}: ${cached.isOnline}`);
      return cached.isOnline;
    }
    
    return false;
  }
}

/**
 * OTIMIZA√á√ÉO FASE 3: Verifica√ß√£o paralela de servi√ßos
 */
async function checkServicesInParallel(): Promise<{ localAI: boolean; n8n: boolean }> {
  logger.debug('üöÄ Iniciando verifica√ß√£o paralela de servi√ßos...');
  
  const checks = await Promise.allSettled([
    checkServiceWithCache('localai', isLocalAIOnline),
    // Adicionar verifica√ß√£o N8N quando dispon√≠vel
    Promise.resolve(true) // N8N sempre considerado dispon√≠vel por enquanto
  ]);
  
  const localAI = checks[0].status === 'fulfilled' ? checks[0].value : false;
  const n8n = checks[1].status === 'fulfilled' ? checks[1].value : true;
  
  logger.debug(`üéØ Resultado verifica√ß√£o: Local AI=${localAI}, N8N=${n8n}`);
  
  return { localAI, n8n };
}

/**
 * Carrega os modelos do arquivo ai_models.json
 */
function loadAIModels(): AIModel[] {
  try {
    const configPath = join(process.cwd(), 'ai_models.json');
    const configContent = readFileSync(configPath, 'utf-8');
    const config: AIModelsConfig = JSON.parse(configContent);
    return config.models;
  } catch (error) {
    logger.error('Erro ao carregar ai_models.json:', error);
    return [];
  }
}

/**
 * Obt√©m a lista de modelos dispon√≠veis baseado na disponibilidade do servi√ßo local
 * OTIMIZA√á√ÉO FASE 3: Verifica√ß√£o paralela e cache de status
 * Limitado a 25 op√ß√µes (limite do Discord)
 */
export async function getAvailableModelOptions(): Promise<ModelOption[]> {
  try {
    logger.debug('üîç Obtendo op√ß√µes de modelos dispon√≠veis...');
    
    // OTIMIZA√á√ÉO FASE 3: Verifica√ß√£o paralela de servi√ßos
    const { localAI: localOnline } = await checkServicesInParallel();
    
    if (localOnline) {
      // Se local est√° online, usar apenas modelos locais
      logger.debug('üñ•Ô∏è Usando modelos locais');
      const localModels = await getAvailableModels();
      
      const options = localModels.slice(0, 24).map(model => ({
        name: `üñ•Ô∏è ${model.name}`.substring(0, 100), // Limite do Discord
        value: model.slug,
        description: model.description?.substring(0, 100) || 'Modelo local'
      }));

      // Adicionar op√ß√£o auto se ainda houver espa√ßo
      if (options.length < 25) {
        options.unshift({
          name: 'ü§ñ Auto-sele√ß√£o',
          value: 'auto',
          description: 'Deixe o sistema escolher o melhor modelo local'
        });
      }

      logger.debug(`‚úÖ ${options.length} modelo(s) local(is) dispon√≠vel(is)`);
      return options;
    } else {
      // Se local est√° offline, usar modelos do N8N (ai_models.json)
      logger.debug('üåê Usando modelos N8N (fallback)');
      const aiModels = loadAIModels();
      
      const options = aiModels.slice(0, 24).map(model => ({
        name: `‚òÅÔ∏è ${model.name}`.substring(0, 100),
        value: model.slug,
        description: model.bestFor?.substring(0, 100) || 'Modelo externo'
      }));

      // Adicionar op√ß√£o auto se ainda houver espa√ßo
      if (options.length < 25) {
        options.unshift({
          name: 'ü§ñ Auto-sele√ß√£o',
          value: 'auto',
          description: 'Deixe o sistema escolher o melhor modelo'
        });
      }

      return options;
    }
  } catch (error) {
    logger.error('Erro ao obter modelos dispon√≠veis:', error);
    
    // Fallback para auto-sele√ß√£o
    return [
      {
        name: 'ü§ñ Auto-sele√ß√£o',
        value: 'auto',
        description: 'Deixe o sistema escolher o melhor modelo'
      }
    ];
  }
}

/**
 * Obt√©m o modelo padr√£o baseado na disponibilidade
 */
export async function getDefaultModel(): Promise<string> {
  try {
    const localOnline = await isLocalAIOnline();
    
    if (localOnline) {
      const localModels = await getAvailableModels();
      return localModels.length > 0 ? localModels[0].slug : 'stable-diffusion-v1.5';
    } else {
      const aiModels = loadAIModels();
      return aiModels.length > 0 ? aiModels[0].slug : 'auto';
    }
  } catch (error) {
    logger.error('Erro ao obter modelo padr√£o:', error);
    return 'auto';
  }
}

/**
 * Verifica se um modelo espec√≠fico est√° dispon√≠vel
 */
export async function isModelAvailable(modelSlug: string): Promise<boolean> {
  try {
    const availableOptions = await getAvailableModelOptions();
    return availableOptions.some(option => option.value === modelSlug);
  } catch (error) {
    logger.error('Erro ao verificar disponibilidade do modelo:', error);
    return false;
  }
}
